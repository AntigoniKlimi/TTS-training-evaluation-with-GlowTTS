{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4dbe5b77",
      "metadata": {
        "id": "4dbe5b77"
      },
      "source": [
        "# Glow-TTS Training 02"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KVVqhIefXc10",
      "metadata": {
        "id": "KVVqhIefXc10"
      },
      "source": [
        "## Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bE8z5rXqesH_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE8z5rXqesH_",
        "outputId": "fb9bef6e-ceda-4a4d-83d9-35558e9d1ce5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount google drive for data storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56af6ad4",
      "metadata": {
        "id": "56af6ad4"
      },
      "source": [
        "## Install Coqui TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fa2aec78",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-06-17T20:31:04.573216Z",
          "iopub.status.busy": "2024-06-17T20:31:04.572353Z",
          "iopub.status.idle": "2024-06-17T20:31:51.176383Z",
          "shell.execute_reply": "2024-06-17T20:31:51.175162Z",
          "shell.execute_reply.started": "2024-06-17T20:31:04.573184Z"
        },
        "id": "fa2aec78",
        "outputId": "1d68d24f-b8a8-4582-89e0-f90095427398",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.1\n",
            "Collecting TTS\n",
            "  Downloading TTS-0.22.0-cp310-cp310-manylinux1_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: cython>=0.29.30 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.0.10)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from TTS) (1.11.4)\n",
            "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from TTS) (2.3.0+cu121)\n",
            "Requirement already satisfied: soundfile>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.12.1)\n",
            "Requirement already satisfied: librosa>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.10.2.post1)\n",
            "Collecting scikit-learn>=1.3.0 (from TTS)\n",
            "  Downloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: inflect>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (7.0.0)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (4.66.4)\n",
            "Collecting anyascii>=0.3.0 (from TTS)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (6.0.1)\n",
            "Requirement already satisfied: fsspec>=2023.6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.9.5)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (24.1)\n",
            "Requirement already satisfied: flask>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (2.2.5)\n",
            "Collecting pysbd>=0.3.4 (from TTS)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting umap-learn>=0.5.1 (from TTS)\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pandas<2.0,>=1.4 (from TTS)\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.7.1)\n",
            "Collecting trainer>=0.0.32 (from TTS)\n",
            "  Downloading trainer-0.0.36-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting coqpit>=0.0.16 (from TTS)\n",
            "  Downloading coqpit-0.0.17-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from TTS) (0.42.1)\n",
            "Collecting pypinyin (from TTS)\n",
            "  Downloading pypinyin-0.51.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting hangul-romanize (from TTS)\n",
            "  Downloading hangul_romanize-0.1.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting gruut==2.2.3 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut-2.2.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jamo (from TTS)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from TTS) (3.8.1)\n",
            "Collecting g2pkk>=0.1.1 (from TTS)\n",
            "  Downloading g2pkk-0.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting bangla (from TTS)\n",
            "  Downloading bangla-0.0.2-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting bnnumerizer (from TTS)\n",
            "  Downloading bnnumerizer-0.0.2.tar.gz (4.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bnunicodenormalizer (from TTS)\n",
            "  Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting einops>=0.6.0 (from TTS)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: transformers>=4.33.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (4.41.2)\n",
            "Collecting encodec>=0.1.1 (from TTS)\n",
            "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidecode>=1.3.2 (from TTS)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting num2words (from TTS)\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: spacy>=3 in /usr/local/lib/python3.10/dist-packages (from spacy[ja]>=3->TTS) (3.7.5)\n",
            "Collecting numpy==1.22.0 (from TTS)\n",
            "  Downloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: numba>=0.57.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.58.1)\n",
            "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (2.15.0)\n",
            "Collecting dateparser~=1.1.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading dateparser-1.1.8-py2.py3-none-any.whl.metadata (27 kB)\n",
            "Collecting gruut-ipa<1.0,>=0.12.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut-ipa-0.13.0.tar.gz (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_en~=2.0.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_en-2.0.0.tar.gz (15.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonlines~=1.2.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting networkx<3.0.0,>=2.5.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting python-crfsuite~=0.9.7 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting gruut_lang_de~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_de-2.0.0.tar.gz (18.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_fr~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_fr-2.0.2.tar.gz (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_es~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_es-2.0.0.tar.gz (31.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (4.0.3)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (8.1.7)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect>=5.6.0->TTS) (2.7.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from inflect>=5.6.0->TTS) (4.12.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (3.0.1)\n",
            "INFO: pip is looking at multiple versions of librosa to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting librosa>=0.10.0 (from TTS)\n",
            "  Downloading librosa-0.10.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "  Downloading librosa-0.10.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Downloading librosa-0.10.0.post2-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Downloading librosa-0.10.0.post1-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Downloading librosa-0.10.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (0.3.7)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (1.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (2.8.2)\n",
            "Collecting docopt>=0.6.2 (from num2words->TTS)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57.0->TTS) (0.41.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0,>=1.4->TTS) (2023.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->TTS) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.0->TTS) (1.16.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (8.2.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.4.0)\n",
            "Collecting sudachipy!=0.6.1,>=0.5.2 (from spacy[ja]>=3->TTS)\n",
            "  Downloading SudachiPy-0.6.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting sudachidict-core>=20211220 (from spacy[ja]>=3->TTS)\n",
            "  Downloading SudachiDict_core-20240409-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->TTS) (3.15.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->TTS) (1.12.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->TTS) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1->TTS)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from trainer>=0.0.32->TTS) (5.9.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from trainer>=0.0.32->TTS) (2.15.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->TTS) (0.23.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->TTS) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->TTS) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->TTS) (0.4.3)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.1->TTS)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.0->TTS) (2.22)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser~=1.1.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask>=2.0.1->TTS) (2.1.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from jsonlines~=1.2.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (1.16.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3->spacy[ja]>=3->TTS) (1.2.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa>=0.10.0->TTS) (4.2.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect>=5.6.0->TTS) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect>=5.6.0->TTS) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3->spacy[ja]>=3->TTS) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3->spacy[ja]>=3->TTS) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (7.0.4)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1->TTS) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.64.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (3.20.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (0.7.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->trainer>=0.0.32->TTS) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->trainer>=0.0.32->TTS) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->trainer>=0.0.32->TTS) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->trainer>=0.0.32->TTS) (1.3.1)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3->spacy[ja]>=3->TTS) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->trainer>=0.0.32->TTS) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->trainer>=0.0.32->TTS) (3.2.2)\n",
            "Downloading TTS-0.22.0-cp310-cp310-manylinux1_x86_64.whl (938 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m938.0/938.0 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coqpit-0.0.17-py3-none-any.whl (13 kB)\n",
            "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading g2pkk-0.1.2-py3-none-any.whl (25 kB)\n",
            "Downloading librosa-0.10.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.9/252.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trainer-0.0.36-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bangla-0.0.2-py2.py3-none-any.whl (6.2 kB)\n",
            "Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl (23 kB)\n",
            "Downloading hangul_romanize-0.1.0-py3-none-any.whl (4.6 kB)\n",
            "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading pypinyin-0.51.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SudachiDict_core-20240409-py3-none-any.whl (72.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SudachiPy-0.6.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gruut, encodec, bnnumerizer, docopt, gruut-ipa, gruut_lang_de, gruut_lang_en, gruut_lang_es, gruut_lang_fr\n",
            "  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut: filename=gruut-2.2.3-py3-none-any.whl size=75791 sha256=8f5385f686076692ba3f5840c0cc4512c093e828e6268bc90b1b5ba9e7fe2c9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/57/a8/f9de532daf5214f53644f20f3a9e6f69269453c87df9c0a817\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45762 sha256=02bd32306e5a32f0f76968f4c2df724595234887471016495a4b21fe79697ff0\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/36/cb/81af8b985a5f5e0815312d5e52b41263237af07b977e6bcbf3\n",
            "  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bnnumerizer: filename=bnnumerizer-0.0.2-py3-none-any.whl size=5261 sha256=193c3c34dc806d81e31948373aea0d99474c4c94b9666ba7d9b0b733c11b3440\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/6b/e8/223172e7d5c9f72df3ea1a0d9258f3a8ab5b28e827728edef5\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=5e11bc038b61107c1332af294d5c934c3d6412b4c807622e8a65b40c095e0ca6\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-ipa: filename=gruut_ipa-0.13.0-py3-none-any.whl size=104870 sha256=6f6f2ff9450edd2238022c0c7f934ee34b98da26dd211a97b225ac8a99cfdb3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/18/49/e4f500ecdf0babe757953f844e4d7cd1ea81c5503c09bfe984\n",
            "  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_de: filename=gruut_lang_de-2.0.0-py3-none-any.whl size=18498183 sha256=45daa41e19b18b432e961a37b0fa23e563e39fd970dee137b9dec67fdf08b2d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/9a/05/cfce98f0c41a1a540f15708c4a02df190b82d84cf91ef6bc7f\n",
            "  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_en: filename=gruut_lang_en-2.0.0-py3-none-any.whl size=15297178 sha256=07408140805d298e8674c2d0759bc1c8cfa45407c0eff0c0043b4245009aa0bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/9c/fb/77c655a9fbd78cdb9935d0ab65d80ddd0a3bcf7dbe18261650\n",
            "  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_es: filename=gruut_lang_es-2.0.0-py3-none-any.whl size=32173797 sha256=b11a189614a982682975b80e14f6acd77a9e5f816bd840394c36f862961b9d20\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/0a/90/788d92c07744b329b9283e37b29b064f5db6b1bb0442a1a19b\n",
            "  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_fr: filename=gruut_lang_fr-2.0.2-py3-none-any.whl size=10968767 sha256=6bd57bacc59f65d73522f89863d10ad2e7ed4c83145cd71c2b2e2463d7d43f3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/21/be/d0436e3f1cf9bf38b9bb9b4a476399c77a1ab19f7172b45e19\n",
            "Successfully built gruut encodec bnnumerizer docopt gruut-ipa gruut_lang_de gruut_lang_en gruut_lang_es gruut_lang_fr\n",
            "Installing collected packages: sudachipy, python-crfsuite, jamo, hangul-romanize, gruut_lang_fr, gruut_lang_es, gruut_lang_en, gruut_lang_de, docopt, bnunicodenormalizer, bnnumerizer, bangla, unidecode, sudachidict-core, pysbd, pypinyin, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, num2words, networkx, jsonlines, gruut-ipa, einops, coqpit, anyascii, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, g2pkk, dateparser, scikit-learn, nvidia-cusolver-cu12, gruut, pynndescent, librosa, umap-learn, trainer, encodec, TTS\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.3\n",
            "    Uninstalling networkx-3.3:\n",
            "      Successfully uninstalled networkx-3.3\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.3\n",
            "    Uninstalling pandas-2.0.3:\n",
            "      Successfully uninstalled pandas-2.0.3\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.10.2.post1\n",
            "    Uninstalling librosa-0.10.2.post1:\n",
            "      Successfully uninstalled librosa-0.10.2.post1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.22.0 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 1.5.3 which is incompatible.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.22.0 which is incompatible.\n",
            "plotnine 0.12.4 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\n",
            "pywavelets 1.6.0 requires numpy<3,>=1.22.4, but you have numpy 1.22.0 which is incompatible.\n",
            "rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "statsmodels 0.14.2 requires numpy>=1.22.3, but you have numpy 1.22.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed TTS-0.22.0 anyascii-0.3.2 bangla-0.0.2 bnnumerizer-0.0.2 bnunicodenormalizer-0.1.7 coqpit-0.0.17 dateparser-1.1.8 docopt-0.6.2 einops-0.8.0 encodec-0.1.1 g2pkk-0.1.2 gruut-2.2.3 gruut-ipa-0.13.0 gruut_lang_de-2.0.0 gruut_lang_en-2.0.0 gruut_lang_es-2.0.0 gruut_lang_fr-2.0.2 hangul-romanize-0.1.0 jamo-0.4.1 jsonlines-1.2.0 librosa-0.10.0 networkx-2.8.8 num2words-0.5.13 numpy-1.22.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pandas-1.5.3 pynndescent-0.5.13 pypinyin-0.51.0 pysbd-0.3.4 python-crfsuite-0.9.10 scikit-learn-1.5.0 sudachidict-core-20240409 sudachipy-0.6.8 trainer-0.0.36 umap-learn-0.5.6 unidecode-1.3.8\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "4c3e3b9645e242438e3536f16deca5c5",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Collecting h5py>=3.10.0 (from tensorflow)\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting keras>=3.0.0 (from tensorflow)\n",
            "  Downloading keras-3.3.3-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Collecting numpy<2.0.0,>=1.23.5 (from tensorflow)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Collecting namex (from keras>=3.0.0->tensorflow)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
            "Collecting optree (from keras>=3.0.0->tensorflow)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: namex, optree, numpy, tensorboard, ml-dtypes, h5py, keras, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.0\n",
            "    Uninstalling numpy-1.22.0:\n",
            "      Successfully uninstalled numpy-1.22.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tts 0.22.0 requires numpy==1.22.0; python_version <= \"3.10\", but you have numpy 1.26.4 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h5py-3.11.0 keras-3.3.3 ml-dtypes-0.3.2 namex-0.0.8 numpy-1.26.4 optree-0.11.0 tensorboard-2.16.2 tensorflow-2.16.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# upgrade pip, install tts, and upgrade tensorflow\n",
        "! pip install -U pip\n",
        "! pip install TTS\n",
        "! pip install -U tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c8eaa71",
      "metadata": {
        "id": "0c8eaa71"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "97b30b4d",
      "metadata": {
        "id": "97b30b4d"
      },
      "outputs": [],
      "source": [
        "# import the necessary libraries\n",
        "import os\n",
        "import glob\n",
        "\n",
        "from trainer import Trainer, TrainerArgs\n",
        "\n",
        "from TTS.utils.audio import AudioProcessor\n",
        "from TTS.tts.models.glow_tts import GlowTTS\n",
        "from TTS.tts.datasets import load_tts_samples\n",
        "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
        "from TTS.tts.configs.glow_tts_config import GlowTTSConfig\n",
        "from TTS.tts.configs.shared_configs import BaseDatasetConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "357c4e0e",
      "metadata": {
        "id": "357c4e0e"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b3cb0191-b8fc-4158-bd26-8423c2a8ba66",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-17T20:31:51.179342Z",
          "iopub.status.busy": "2024-06-17T20:31:51.178913Z",
          "iopub.status.idle": "2024-06-17T20:31:53.177657Z",
          "shell.execute_reply": "2024-06-17T20:31:53.176795Z",
          "shell.execute_reply.started": "2024-06-17T20:31:51.179290Z"
        },
        "id": "b3cb0191-b8fc-4158-bd26-8423c2a8ba66",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# set output path under google drive directory\n",
        "output_path = \"/content/drive/MyDrive/ljspeech-002/tts_train_dir\"\n",
        "if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)\n",
        "\n",
        "# set the dataset configuration\n",
        "# ljspeech dataset was chosen, but only one speaker (002) will be used\n",
        "dataset_config = BaseDatasetConfig(\n",
        "    formatter=\"ljspeech\", meta_file_train=\"/content/drive/MyDrive/ljspeech-002/metadata.csv\", path=\"/content/drive/MyDrive/ljspeech-002\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96ab70e8",
      "metadata": {
        "id": "96ab70e8"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5483ca28-39d6-49f8-a18e-4fb53c50ad84",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-17T20:31:53.187133Z",
          "iopub.status.busy": "2024-06-17T20:31:53.186857Z",
          "iopub.status.idle": "2024-06-17T20:31:53.217943Z",
          "shell.execute_reply": "2024-06-17T20:31:53.216972Z",
          "shell.execute_reply.started": "2024-06-17T20:31:53.187109Z"
        },
        "id": "5483ca28-39d6-49f8-a18e-4fb53c50ad84",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# model architecture and configuration\n",
        "# 250 epochs will be trained with 32 training batch size\n",
        "# checkpoints every 200 steps, meaning around every 19 epochs,\n",
        "# since 337 training samples / 32 batch size = 10.5 steps per epoch\n",
        "config = GlowTTSConfig(\n",
        "    batch_size=32,\n",
        "    eval_batch_size=16,\n",
        "    num_loader_workers=4,\n",
        "    num_eval_loader_workers=4,\n",
        "    run_eval=True,\n",
        "    test_delay_epochs=-1,\n",
        "    epochs=250,\n",
        "    text_cleaner=\"phoneme_cleaners\",\n",
        "    use_phonemes=True,\n",
        "    phoneme_language=\"en-us\",\n",
        "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
        "    print_step=25,\n",
        "    print_eval=False,\n",
        "    mixed_precision=True,\n",
        "    output_path=output_path,\n",
        "    datasets=[dataset_config],\n",
        "    save_step=200,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b93ed377-80b7-447b-bd92-106bffa777ee",
      "metadata": {
        "id": "b93ed377-80b7-447b-bd92-106bffa777ee"
      },
      "source": [
        "## Audio Processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b1b12f61-f851-4565-84dd-7640947e04ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-17T20:31:53.219259Z",
          "iopub.status.busy": "2024-06-17T20:31:53.218985Z",
          "iopub.status.idle": "2024-06-17T20:31:55.395938Z",
          "shell.execute_reply": "2024-06-17T20:31:55.394968Z",
          "shell.execute_reply.started": "2024-06-17T20:31:53.219235Z"
        },
        "id": "b1b12f61-f851-4565-84dd-7640947e04ab",
        "outputId": "9c0260b3-1b8e-46f9-93a7-a57aa7990180",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:-100\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:20\n",
            " | > fft_size:1024\n",
            " | > power:1.5\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:60\n",
            " | > signal_norm:True\n",
            " | > symmetric_norm:True\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:1.0\n",
            " | > pitch_fmax:640.0\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:4.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:True\n",
            " | > trim_db:45\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n"
          ]
        }
      ],
      "source": [
        "# audio processor for feature extraction and audio input/output\n",
        "ap = AudioProcessor.init_from_config(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d461683-b05e-403f-815f-8007bda08c38",
      "metadata": {
        "id": "1d461683-b05e-403f-815f-8007bda08c38"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "014879b7-f18d-44c0-b24a-e10f8002113a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-17T20:31:55.397546Z",
          "iopub.status.busy": "2024-06-17T20:31:55.397054Z",
          "iopub.status.idle": "2024-06-17T20:32:00.227234Z",
          "shell.execute_reply": "2024-06-17T20:32:00.226398Z",
          "shell.execute_reply.started": "2024-06-17T20:31:55.397516Z"
        },
        "id": "014879b7-f18d-44c0-b24a-e10f8002113a",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# tokenizer for converting text to sequences of token ids\n",
        "tokenizer, config = TTSTokenizer.init_from_config(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df3016e1-9e99-4c4f-94e3-fa89231fd978",
      "metadata": {
        "id": "df3016e1-9e99-4c4f-94e3-fa89231fd978"
      },
      "source": [
        "## Load Data Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cadd6ada-c8eb-4f79-b8fe-6d72850af5a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-17T20:32:00.229151Z",
          "iopub.status.busy": "2024-06-17T20:32:00.228530Z",
          "iopub.status.idle": "2024-06-17T20:32:00.568750Z",
          "shell.execute_reply": "2024-06-17T20:32:00.567768Z",
          "shell.execute_reply.started": "2024-06-17T20:32:00.229114Z"
        },
        "id": "cadd6ada-c8eb-4f79-b8fe-6d72850af5a7",
        "outputId": "925262c2-07ff-4be5-948e-7a8c7b733b4e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Found 337 files in /content/drive/MyDrive/ljspeech-002\n"
          ]
        }
      ],
      "source": [
        "# load the training and evaluation samples from the dataset\n",
        "# each sample is a list of [text, audio_file_path, speaker_name]\n",
        "train_samples, eval_samples = load_tts_samples(\n",
        "    dataset_config,\n",
        "    eval_split=True,\n",
        "    eval_split_max_size=config.eval_split_max_size,\n",
        "    eval_split_size=config.eval_split_size,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db8b451e-1fe1-4aa3-b69e-ab22b925bd19",
      "metadata": {
        "id": "db8b451e-1fe1-4aa3-b69e-ab22b925bd19"
      },
      "source": [
        "## Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0f609945-4fe0-4d0d-b95e-11d7bfb63ebe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-17T20:32:01.295471Z",
          "iopub.status.busy": "2024-06-17T20:32:01.294832Z",
          "iopub.status.idle": "2024-06-17T20:32:05.789353Z",
          "shell.execute_reply": "2024-06-17T20:32:05.788343Z",
          "shell.execute_reply.started": "2024-06-17T20:32:01.295415Z"
        },
        "id": "0f609945-4fe0-4d0d-b95e-11d7bfb63ebe",
        "outputId": "c11e2757-2a1f-4128-c923-b2a4f5059970",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " > Training Environment:\n",
            " | > Backend: Torch\n",
            " | > Mixed precision: True\n",
            " | > Precision: fp16\n",
            " | > Num. of CPUs: 2\n",
            " | > Num. of Torch Threads: 1\n",
            " | > Torch seed: 54321\n",
            " | > Torch CUDNN: True\n",
            " | > Torch CUDNN deterministic: False\n",
            " | > Torch CUDNN benchmark: False\n",
            " | > Torch TF32 MatMul: False\n",
            " > Start Tensorboard: tensorboard --logdir=/content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_01+45PM-0000000\n",
            " > Restoring from checkpoint_2400.pth ...\n",
            " > Restoring Model...\n",
            " > Restoring Optimizer...\n",
            " > Model restored from step 2400\n",
            "/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "\n",
            " > Model has 28610257 parameters\n"
          ]
        }
      ],
      "source": [
        "# model initialization with configuration, audio processor, tokenizer, and no speaker manager\n",
        "model = GlowTTS(config, ap, tokenizer, speaker_manager=None)\n",
        "\n",
        "# trainer generic api for training the model\n",
        "# here, we also restore the model from the last checkpoint and continue training,\n",
        "# because it was trained for 2400+ steps before the runtime was disconnected\n",
        "trainer = Trainer(\n",
        "    TrainerArgs(restore_path=output_path+'/run-June-19-2024_05+47PM-0000000/checkpoint_2400.pth'),\n",
        "    config,\n",
        "    output_path,\n",
        "    model=model,\n",
        "    train_samples=train_samples,\n",
        "    eval_samples=eval_samples\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b320831-dd83-429b-bb6a-473f9d49d321",
      "metadata": {
        "id": "5b320831-dd83-429b-bb6a-473f9d49d321"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4c07f99-3d1d-4bea-801e-9f33bbff0e9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-06-17T20:32:05.792249Z",
          "iopub.status.busy": "2024-06-17T20:32:05.791931Z",
          "iopub.status.idle": "2024-06-17T21:12:43.142735Z",
          "shell.execute_reply": "2024-06-17T21:12:43.141536Z",
          "shell.execute_reply.started": "2024-06-17T20:32:05.792223Z"
        },
        "id": "d4c07f99-3d1d-4bea-801e-9f33bbff0e9f",
        "outputId": "8d482645-701d-49bd-d942-200ba39fc68c",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 0/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "| > Number of instances : 334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:12:32) \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Preprocessing samples\n",
            " | > Max text length: 179\n",
            " | > Min text length: 32\n",
            " | > Avg text length: 107.95808383233533\n",
            " | \n",
            " | > Max audio length: 222643.0\n",
            " | > Min audio length: 33971.0\n",
            " | > Avg audio length: 146823.3113772455\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 148\n",
            " | > Min text length: 85\n",
            " | > Avg text length: 125.66666666666667\n",
            " | \n",
            " | > Max audio length: 208051.0\n",
            " | > Min audio length: 103603.0\n",
            " | > Avg audio length: 168029.66666666666\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv1d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 2.6919105052948 \u001b[0m(+0)\n",
            "     | > avg_loss: 0.1961331069469452 \u001b[0m(+0)\n",
            "     | > avg_log_mle: -0.11461901664733887 \u001b[0m(+0)\n",
            "     | > avg_loss_dur: 0.31075212359428406 \u001b[0m(+0)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000/best_model_2412.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 1/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:14:11) \u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5195260047912598 \u001b[0m(-2.17238450050354)\n",
            "     | > avg_loss:\u001b[92m 0.1761244237422943 \u001b[0m(-0.02000868320465088)\n",
            "     | > avg_log_mle:\u001b[92m -0.11624085903167725 \u001b[0m(-0.001621842384338379)\n",
            "     | > avg_loss_dur:\u001b[92m 0.29236528277397156 \u001b[0m(-0.0183868408203125)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000/best_model_2423.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 2/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:14:50) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:15:05 -- STEP: 2/11 -- GLOBAL_STEP: 2425\u001b[0m\n",
            "     | > loss: 0.14706763625144958  (0.13680413365364075)\n",
            "     | > log_mle: -0.1611539125442505  (-0.15940940380096436)\n",
            "     | > loss_dur: 0.3082215487957001  (0.2962135374546051)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(0.7353, device='cuda:0')  (tensor(0.7434, device='cuda:0'))\n",
            "     | > current_lr: 5e-07 \n",
            "     | > step_time: 4.61  (3.340731382369995)\n",
            "     | > loader_time: 0.0134  (0.011153578758239746)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv1d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 1.2559545040130615 \u001b[0m(+0.7364284992218018)\n",
            "     | > avg_loss:\u001b[91m 0.1828700304031372 \u001b[0m(+0.0067456066608428955)\n",
            "     | > avg_log_mle:\u001b[92m -0.11753380298614502 \u001b[0m(-0.0012929439544677734)\n",
            "     | > avg_loss_dur:\u001b[91m 0.3004038333892822 \u001b[0m(+0.008038550615310669)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 3/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:15:30) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.551311731338501 \u001b[0m(-0.7046427726745605)\n",
            "     | > avg_loss:\u001b[91m 0.18820402026176453 \u001b[0m(+0.005333989858627319)\n",
            "     | > avg_log_mle:\u001b[92m -0.11852538585662842 \u001b[0m(-0.0009915828704833984)\n",
            "     | > avg_loss_dur:\u001b[91m 0.30672940611839294 \u001b[0m(+0.006325572729110718)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 4/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:16:05) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:16:21 -- STEP: 5/11 -- GLOBAL_STEP: 2450\u001b[0m\n",
            "     | > loss: 0.18080687522888184  (0.15272245407104493)\n",
            "     | > log_mle: -0.14653337001800537  (-0.15260822772979737)\n",
            "     | > loss_dur: 0.3273402452468872  (0.3053306818008423)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(0.5357, device='cuda:0')  (tensor(0.5732, device='cuda:0'))\n",
            "     | > current_lr: 1e-06 \n",
            "     | > step_time: 1.1974  (2.357244873046875)\n",
            "     | > loader_time: 0.0224  (0.03955225944519043)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.5545444488525391 \u001b[0m(+0.003232717514038086)\n",
            "     | > avg_loss:\u001b[92m 0.18127459287643433 \u001b[0m(-0.0069294273853302)\n",
            "     | > avg_log_mle:\u001b[92m -0.11919271945953369 \u001b[0m(-0.0006673336029052734)\n",
            "     | > avg_loss_dur:\u001b[92m 0.300467312335968 \u001b[0m(-0.006262093782424927)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 5/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:16:39) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.54925537109375 \u001b[0m(-0.0052890777587890625)\n",
            "     | > avg_loss:\u001b[91m 0.18193325400352478 \u001b[0m(+0.0006586611270904541)\n",
            "     | > avg_log_mle:\u001b[91m -0.11915743350982666 \u001b[0m(+3.528594970703125e-05)\n",
            "     | > avg_loss_dur:\u001b[91m 0.30109068751335144 \u001b[0m(+0.0006233751773834229)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 6/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:17:12) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:17:35 -- STEP: 8/11 -- GLOBAL_STEP: 2475\u001b[0m\n",
            "     | > loss: 0.19276639819145203  (0.1658630073070526)\n",
            "     | > log_mle: -0.15391266345977783  (-0.15204587578773499)\n",
            "     | > loss_dur: 0.34667906165122986  (0.3179088830947876)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(1.0321, device='cuda:0')  (tensor(0.6242, device='cuda:0'))\n",
            "     | > current_lr: 1.5e-06 \n",
            "     | > step_time: 0.7252  (1.7648503482341766)\n",
            "     | > loader_time: 0.0079  (0.02746528387069702)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.5760371685028076 \u001b[0m(+0.026781797409057617)\n",
            "     | > avg_loss:\u001b[92m 0.18192172050476074 \u001b[0m(-1.1533498764038086e-05)\n",
            "     | > avg_log_mle:\u001b[91m -0.11911225318908691 \u001b[0m(+4.5180320739746094e-05)\n",
            "     | > avg_loss_dur:\u001b[92m 0.30103397369384766 \u001b[0m(-5.671381950378418e-05)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 7/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:17:51) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5602304935455322 \u001b[0m(-0.01580667495727539)\n",
            "     | > avg_loss:\u001b[91m 0.1823868751525879 \u001b[0m(+0.00046515464782714844)\n",
            "     | > avg_log_mle:\u001b[92m -0.11918103694915771 \u001b[0m(-6.878376007080078e-05)\n",
            "     | > avg_loss_dur:\u001b[91m 0.3015679121017456 \u001b[0m(+0.0005339384078979492)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 8/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:18:25) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.558659553527832 \u001b[0m(-0.0015709400177001953)\n",
            "     | > avg_loss:\u001b[92m 0.18176355957984924 \u001b[0m(-0.0006233155727386475)\n",
            "     | > avg_log_mle:\u001b[92m -0.11920821666717529 \u001b[0m(-2.7179718017578125e-05)\n",
            "     | > avg_loss_dur:\u001b[92m 0.30097177624702454 \u001b[0m(-0.0005961358547210693)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 9/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:18:57) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:19:01 -- STEP: 0/11 -- GLOBAL_STEP: 2500\u001b[0m\n",
            "     | > loss: 0.07789146900177002  (0.07789146900177002)\n",
            "     | > log_mle: -0.1699584722518921  (-0.1699584722518921)\n",
            "     | > loss_dur: 0.2478499412536621  (0.2478499412536621)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(0.6871, device='cuda:0')  (tensor(0.6871, device='cuda:0'))\n",
            "     | > current_lr: 2.25e-06 \n",
            "     | > step_time: 1.5467  (1.5467231273651123)\n",
            "     | > loader_time: 2.5303  (2.5302817821502686)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.6014444828033447 \u001b[0m(+0.042784929275512695)\n",
            "     | > avg_loss:\u001b[92m 0.17590966820716858 \u001b[0m(-0.005853891372680664)\n",
            "     | > avg_log_mle:\u001b[92m -0.11932647228240967 \u001b[0m(-0.000118255615234375)\n",
            "     | > avg_loss_dur:\u001b[92m 0.29523614048957825 \u001b[0m(-0.005735635757446289)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000/best_model_2511.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 10/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:19:35) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5758326053619385 \u001b[0m(-0.02561187744140625)\n",
            "     | > avg_loss:\u001b[91m 0.17651447653770447 \u001b[0m(+0.0006048083305358887)\n",
            "     | > avg_log_mle:\u001b[91m -0.11931633949279785 \u001b[0m(+1.0132789611816406e-05)\n",
            "     | > avg_loss_dur:\u001b[91m 0.2958308160305023 \u001b[0m(+0.0005946755409240723)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 11/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:20:12) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:20:26 -- STEP: 3/11 -- GLOBAL_STEP: 2525\u001b[0m\n",
            "     | > loss: 0.14482605457305908  (0.13968878984451294)\n",
            "     | > log_mle: -0.14788389205932617  (-0.1559940973917643)\n",
            "     | > loss_dur: 0.29270994663238525  (0.2956828872362773)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(1.1650, device='cuda:0')  (tensor(0.9243, device='cuda:0'))\n",
            "     | > current_lr: 2.75e-06 \n",
            "     | > step_time: 2.7049  (2.8809019724527993)\n",
            "     | > loader_time: 0.0567  (0.04021914800008138)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.585282564163208 \u001b[0m(+0.009449958801269531)\n",
            "     | > avg_loss:\u001b[91m 0.17678967118263245 \u001b[0m(+0.0002751946449279785)\n",
            "     | > avg_log_mle:\u001b[92m -0.11941039562225342 \u001b[0m(-9.40561294555664e-05)\n",
            "     | > avg_loss_dur:\u001b[91m 0.29620006680488586 \u001b[0m(+0.0003692507743835449)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 12/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:20:49) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.6069920063018799 \u001b[0m(+0.021709442138671875)\n",
            "     | > avg_loss:\u001b[92m 0.17542937397956848 \u001b[0m(-0.0013602972030639648)\n",
            "     | > avg_log_mle:\u001b[92m -0.11943626403808594 \u001b[0m(-2.586841583251953e-05)\n",
            "     | > avg_loss_dur:\u001b[92m 0.2948656380176544 \u001b[0m(-0.0013344287872314453)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000/best_model_2544.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 13/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:21:26) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:21:45 -- STEP: 6/11 -- GLOBAL_STEP: 2550\u001b[0m\n",
            "     | > loss: 0.17748987674713135  (0.15564062694708505)\n",
            "     | > log_mle: -0.14875197410583496  (-0.15243146816889444)\n",
            "     | > loss_dur: 0.3262418508529663  (0.3080720951159795)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(0.5580, device='cuda:0')  (tensor(0.6288, device='cuda:0'))\n",
            "     | > current_lr: 3.25e-06 \n",
            "     | > step_time: 0.724  (1.8690972725550334)\n",
            "     | > loader_time: 0.0083  (0.039642532666524254)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5989959239959717 \u001b[0m(-0.007996082305908203)\n",
            "     | > avg_loss:\u001b[91m 0.17647713422775269 \u001b[0m(+0.001047760248184204)\n",
            "     | > avg_log_mle:\u001b[92m -0.11952710151672363 \u001b[0m(-9.083747863769531e-05)\n",
            "     | > avg_loss_dur:\u001b[91m 0.2960042357444763 \u001b[0m(+0.0011385977268218994)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 14/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:22:02) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5655233860015869 \u001b[0m(-0.033472537994384766)\n",
            "     | > avg_loss:\u001b[92m 0.17548036575317383 \u001b[0m(-0.0009967684745788574)\n",
            "     | > avg_log_mle:\u001b[92m -0.11955392360687256 \u001b[0m(-2.682209014892578e-05)\n",
            "     | > avg_loss_dur:\u001b[92m 0.2950342893600464 \u001b[0m(-0.0009699463844299316)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 15/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:22:36) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:22:58 -- STEP: 9/11 -- GLOBAL_STEP: 2575\u001b[0m\n",
            "     | > loss: 0.18327438831329346  (0.16579846209949917)\n",
            "     | > log_mle: -0.14632153511047363  (-0.1520196861690945)\n",
            "     | > loss_dur: 0.3295959234237671  (0.31781814826859367)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(0.9008, device='cuda:0')  (tensor(0.8514, device='cuda:0'))\n",
            "     | > current_lr: 3.7499999999999997e-06 \n",
            "     | > step_time: 0.7882  (1.2025850613911946)\n",
            "     | > loader_time: 0.0103  (0.01384515232510037)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 1.5689778327941895 \u001b[0m(+1.0034544467926025)\n",
            "     | > avg_loss:\u001b[92m 0.17407622933387756 \u001b[0m(-0.0014041364192962646)\n",
            "     | > avg_log_mle:\u001b[92m -0.1198737621307373 \u001b[0m(-0.0003198385238647461)\n",
            "     | > avg_loss_dur:\u001b[92m 0.29394999146461487 \u001b[0m(-0.0010842978954315186)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000/best_model_2577.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 16/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:23:28) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.7588098049163818 \u001b[0m(-0.8101680278778076)\n",
            "     | > avg_loss:\u001b[92m 0.17267978191375732 \u001b[0m(-0.0013964474201202393)\n",
            "     | > avg_log_mle:\u001b[92m -0.11995577812194824 \u001b[0m(-8.20159912109375e-05)\n",
            "     | > avg_loss_dur:\u001b[92m 0.29263556003570557 \u001b[0m(-0.0013144314289093018)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000/best_model_2588.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 17/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:24:11) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5687947273254395 \u001b[0m(-0.19001507759094238)\n",
            "     | > avg_loss:\u001b[92m 0.171473890542984 \u001b[0m(-0.0012058913707733154)\n",
            "     | > avg_log_mle:\u001b[92m -0.12027299404144287 \u001b[0m(-0.0003172159194946289)\n",
            "     | > avg_loss_dur:\u001b[92m 0.2917468845844269 \u001b[0m(-0.0008886754512786865)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000/best_model_2599.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 18/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:24:54) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:24:59 -- STEP: 1/11 -- GLOBAL_STEP: 2600\u001b[0m\n",
            "     | > loss: 0.1235416829586029  (0.1235416829586029)\n",
            "     | > log_mle: -0.15907800197601318  (-0.15907800197601318)\n",
            "     | > loss_dur: 0.2826196849346161  (0.2826196849346161)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(1.0844, device='cuda:0')  (tensor(1.0844, device='cuda:0'))\n",
            "     | > current_lr: 4.5e-06 \n",
            "     | > step_time: 1.004  (1.0040297508239746)\n",
            "     | > loader_time: 0.0244  (0.024408817291259766)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000/checkpoint_2600.pth\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 1.5835490226745605 \u001b[0m(+1.014754295349121)\n",
            "     | > avg_loss:\u001b[91m 0.171816885471344 \u001b[0m(+0.00034299492835998535)\n",
            "     | > avg_log_mle:\u001b[92m -0.12036705017089844 \u001b[0m(-9.40561294555664e-05)\n",
            "     | > avg_loss_dur:\u001b[91m 0.29218393564224243 \u001b[0m(+0.00043705105781555176)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 19/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:25:35) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.6018016338348389 \u001b[0m(-0.9817473888397217)\n",
            "     | > avg_loss:\u001b[92m 0.16694530844688416 \u001b[0m(-0.004871577024459839)\n",
            "     | > avg_log_mle:\u001b[92m -0.12066507339477539 \u001b[0m(-0.0002980232238769531)\n",
            "     | > avg_loss_dur:\u001b[92m 0.28761038184165955 \u001b[0m(-0.004573553800582886)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000/best_model_2621.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 20/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:26:14) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:26:29 -- STEP: 4/11 -- GLOBAL_STEP: 2625\u001b[0m\n",
            "     | > loss: 0.1694798767566681  (0.14297743886709213)\n",
            "     | > log_mle: -0.15152478218078613  (-0.1559045910835266)\n",
            "     | > loss_dur: 0.3210046589374542  (0.29888202995061874)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(1.0325, device='cuda:0')  (tensor(1.3998, device='cuda:0'))\n",
            "     | > current_lr: 4.9999999999999996e-06 \n",
            "     | > step_time: 1.7637  (1.6727542877197266)\n",
            "     | > loader_time: 0.043  (0.021675169467926025)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.8585488796234131 \u001b[0m(+0.2567472457885742)\n",
            "     | > avg_loss:\u001b[91m 0.167738139629364 \u001b[0m(+0.0007928311824798584)\n",
            "     | > avg_log_mle:\u001b[92m -0.12072980403900146 \u001b[0m(-6.473064422607422e-05)\n",
            "     | > avg_loss_dur:\u001b[91m 0.2884679436683655 \u001b[0m(+0.0008575618267059326)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 21/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:26:50) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5863780975341797 \u001b[0m(-0.2721707820892334)\n",
            "     | > avg_loss:\u001b[91m 0.17357438802719116 \u001b[0m(+0.0058362483978271484)\n",
            "     | > avg_log_mle:\u001b[91m -0.12067306041717529 \u001b[0m(+5.6743621826171875e-05)\n",
            "     | > avg_loss_dur:\u001b[91m 0.29424744844436646 \u001b[0m(+0.0057795047760009766)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 22/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:27:21) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:27:35 -- STEP: 7/11 -- GLOBAL_STEP: 2650\u001b[0m\n",
            "     | > loss: 0.17587339878082275  (0.1557975709438324)\n",
            "     | > log_mle: -0.15286004543304443  (-0.1535085780279977)\n",
            "     | > loss_dur: 0.3287334442138672  (0.3093061489718301)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(0.9808, device='cuda:0')  (tensor(1.2589, device='cuda:0'))\n",
            "     | > current_lr: 5.5e-06 \n",
            "     | > step_time: 0.8374  (1.2723926135471888)\n",
            "     | > loader_time: 0.0075  (0.017763274056570872)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5838649272918701 \u001b[0m(-0.0025131702423095703)\n",
            "     | > avg_loss:\u001b[91m 0.17367014288902283 \u001b[0m(+9.575486183166504e-05)\n",
            "     | > avg_log_mle:\u001b[92m -0.12076258659362793 \u001b[0m(-8.952617645263672e-05)\n",
            "     | > avg_loss_dur:\u001b[91m 0.29443272948265076 \u001b[0m(+0.00018528103828430176)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 23/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:27:53) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.599250078201294 \u001b[0m(+0.015385150909423828)\n",
            "     | > avg_loss:\u001b[92m 0.17180365324020386 \u001b[0m(-0.0018664896488189697)\n",
            "     | > avg_log_mle:\u001b[92m -0.12087225914001465 \u001b[0m(-0.00010967254638671875)\n",
            "     | > avg_loss_dur:\u001b[92m 0.2926759123802185 \u001b[0m(-0.001756817102432251)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 24/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:28:29) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:28:45 -- STEP: 10/11 -- GLOBAL_STEP: 2675\u001b[0m\n",
            "     | > loss: 0.13428127765655518  (0.1608154684305191)\n",
            "     | > log_mle: -0.1732633113861084  (-0.15543328523635863)\n",
            "     | > loss_dur: 0.3075445890426636  (0.31624875366687777)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(1.6623, device='cuda:0')  (tensor(1.2505, device='cuda:0'))\n",
            "     | > current_lr: 6e-06 \n",
            "     | > step_time: 0.4942  (1.1066381692886353)\n",
            "     | > loader_time: 0.0062  (0.015817546844482423)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5846967697143555 \u001b[0m(-0.014553308486938477)\n",
            "     | > avg_loss:\u001b[91m 0.17279785871505737 \u001b[0m(+0.0009942054748535156)\n",
            "     | > avg_log_mle:\u001b[91m -0.12076115608215332 \u001b[0m(+0.00011110305786132812)\n",
            "     | > avg_loss_dur:\u001b[91m 0.2935590147972107 \u001b[0m(+0.0008831024169921875)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 25/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:29:01) \u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.6150026321411133 \u001b[0m(+0.030305862426757812)\n",
            "     | > avg_loss:\u001b[92m 0.17173343896865845 \u001b[0m(-0.0010644197463989258)\n",
            "     | > avg_log_mle:\u001b[92m -0.1212085485458374 \u001b[0m(-0.00044739246368408203)\n",
            "     | > avg_loss_dur:\u001b[92m 0.29294198751449585 \u001b[0m(-0.0006170272827148438)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 26/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:29:33) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5886335372924805 \u001b[0m(-0.026369094848632812)\n",
            "     | > avg_loss:\u001b[91m 0.172525554895401 \u001b[0m(+0.0007921159267425537)\n",
            "     | > avg_log_mle:\u001b[92m -0.12146377563476562 \u001b[0m(-0.00025522708892822266)\n",
            "     | > avg_loss_dur:\u001b[91m 0.2939893305301666 \u001b[0m(+0.0010473430156707764)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 27/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:30:04) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:30:11 -- STEP: 2/11 -- GLOBAL_STEP: 2700\u001b[0m\n",
            "     | > loss: 0.149117112159729  (0.1382044404745102)\n",
            "     | > log_mle: -0.16467297077178955  (-0.16285520792007446)\n",
            "     | > loss_dur: 0.31379008293151855  (0.30105964839458466)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(1.1650, device='cuda:0')  (tensor(1.0651, device='cuda:0'))\n",
            "     | > current_lr: 6.75e-06 \n",
            "     | > step_time: 1.858  (1.5029699802398682)\n",
            "     | > loader_time: 0.0162  (0.011566996574401855)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5863897800445557 \u001b[0m(-0.0022437572479248047)\n",
            "     | > avg_loss:\u001b[92m 0.1718067228794098 \u001b[0m(-0.0007188320159912109)\n",
            "     | > avg_log_mle:\u001b[92m -0.12153196334838867 \u001b[0m(-6.818771362304688e-05)\n",
            "     | > avg_loss_dur:\u001b[92m 0.29333868622779846 \u001b[0m(-0.0006506443023681641)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 28/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:30:35) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.5962893962860107 \u001b[0m(+0.009899616241455078)\n",
            "     | > avg_loss:\u001b[92m 0.1717754602432251 \u001b[0m(-3.126263618469238e-05)\n",
            "     | > avg_log_mle:\u001b[92m -0.12181103229522705 \u001b[0m(-0.0002790689468383789)\n",
            "     | > avg_loss_dur:\u001b[91m 0.29358649253845215 \u001b[0m(+0.0002478063106536865)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 29/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:31:06) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:31:18 -- STEP: 5/11 -- GLOBAL_STEP: 2725\u001b[0m\n",
            "     | > loss: 0.16567206382751465  (0.1458720088005066)\n",
            "     | > log_mle: -0.1496291160583496  (-0.15583863258361816)\n",
            "     | > loss_dur: 0.31530117988586426  (0.30171064138412473)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(0.9568, device='cuda:0')  (tensor(1.0591, device='cuda:0'))\n",
            "     | > current_lr: 7.25e-06 \n",
            "     | > step_time: 0.79  (1.5413455486297607)\n",
            "     | > loader_time: 0.0072  (0.020896148681640626)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.6029543876647949 \u001b[0m(+0.00666499137878418)\n",
            "     | > avg_loss:\u001b[92m 0.17140227556228638 \u001b[0m(-0.0003731846809387207)\n",
            "     | > avg_log_mle:\u001b[91m -0.12174010276794434 \u001b[0m(+7.092952728271484e-05)\n",
            "     | > avg_loss_dur:\u001b[92m 0.2931423783302307 \u001b[0m(-0.00044411420822143555)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 30/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:31:40) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5985190868377686 \u001b[0m(-0.004435300827026367)\n",
            "     | > avg_loss:\u001b[92m 0.1709003746509552 \u001b[0m(-0.0005019009113311768)\n",
            "     | > avg_log_mle:\u001b[92m -0.12207949161529541 \u001b[0m(-0.0003393888473510742)\n",
            "     | > avg_loss_dur:\u001b[92m 0.2929798662662506 \u001b[0m(-0.00016251206398010254)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 31/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:32:12) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:32:27 -- STEP: 8/11 -- GLOBAL_STEP: 2750\u001b[0m\n",
            "     | > loss: 0.18160510063171387  (0.1572103574872017)\n",
            "     | > log_mle: -0.15751898288726807  (-0.15566052496433258)\n",
            "     | > loss_dur: 0.33912408351898193  (0.31287088245153427)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(1.0228, device='cuda:0')  (tensor(0.9861, device='cuda:0'))\n",
            "     | > current_lr: 7.75e-06 \n",
            "     | > step_time: 0.7884  (1.3169847428798676)\n",
            "     | > loader_time: 0.0088  (0.017714768648147583)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.6095497608184814 \u001b[0m(+0.01103067398071289)\n",
            "     | > avg_loss:\u001b[92m 0.17034456133842468 \u001b[0m(-0.0005558133125305176)\n",
            "     | > avg_log_mle:\u001b[92m -0.12262403964996338 \u001b[0m(-0.0005445480346679688)\n",
            "     | > avg_loss_dur:\u001b[92m 0.29296860098838806 \u001b[0m(-1.1265277862548828e-05)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 32/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:32:44) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5975315570831299 \u001b[0m(-0.012018203735351562)\n",
            "     | > avg_loss:\u001b[92m 0.1699312925338745 \u001b[0m(-0.0004132688045501709)\n",
            "     | > avg_log_mle:\u001b[92m -0.12284219264984131 \u001b[0m(-0.0002181529998779297)\n",
            "     | > avg_loss_dur:\u001b[92m 0.2927734851837158 \u001b[0m(-0.0001951158046722412)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 33/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:33:14) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5715744495391846 \u001b[0m(-0.025957107543945312)\n",
            "     | > avg_loss:\u001b[91m 0.17008858919143677 \u001b[0m(+0.00015729665756225586)\n",
            "     | > avg_log_mle:\u001b[92m -0.12310516834259033 \u001b[0m(-0.00026297569274902344)\n",
            "     | > avg_loss_dur:\u001b[91m 0.2931937575340271 \u001b[0m(+0.0004202723503112793)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 34/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:33:45) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:33:49 -- STEP: 0/11 -- GLOBAL_STEP: 2775\u001b[0m\n",
            "     | > loss: 0.06086397171020508  (0.06086397171020508)\n",
            "     | > log_mle: -0.17466247081756592  (-0.17466247081756592)\n",
            "     | > loss_dur: 0.235526442527771  (0.235526442527771)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(1.4376, device='cuda:0')  (tensor(1.4376, device='cuda:0'))\n",
            "     | > current_lr: 8.5e-06 \n",
            "     | > step_time: 1.6946  (1.6946032047271729)\n",
            "     | > loader_time: 2.814  (2.8140296936035156)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5704119205474854 \u001b[0m(-0.0011625289916992188)\n",
            "     | > avg_loss:\u001b[91m 0.1735670566558838 \u001b[0m(+0.0034784674644470215)\n",
            "     | > avg_log_mle:\u001b[92m -0.12336695194244385 \u001b[0m(-0.0002617835998535156)\n",
            "     | > avg_loss_dur:\u001b[91m 0.29693400859832764 \u001b[0m(+0.003740251064300537)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 35/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:34:16) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.6102068424224854 \u001b[0m(+0.039794921875)\n",
            "     | > avg_loss:\u001b[92m 0.16845375299453735 \u001b[0m(-0.0051133036613464355)\n",
            "     | > avg_log_mle:\u001b[91m -0.12330853939056396 \u001b[0m(+5.841255187988281e-05)\n",
            "     | > avg_loss_dur:\u001b[92m 0.2917622923851013 \u001b[0m(-0.005171716213226318)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 36/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:34:46) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:34:55 -- STEP: 3/11 -- GLOBAL_STEP: 2800\u001b[0m\n",
            "     | > loss: 0.13212835788726807  (0.12688310941060385)\n",
            "     | > log_mle: -0.15248560905456543  (-0.16029155254364014)\n",
            "     | > loss_dur: 0.2846139669418335  (0.28717466195424396)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(1.4760, device='cuda:0')  (tensor(1.3395, device='cuda:0'))\n",
            "     | > current_lr: 9e-06 \n",
            "     | > step_time: 0.9633  (1.3981742858886719)\n",
            "     | > loader_time: 0.0392  (0.026866912841796875)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000/checkpoint_2800.pth\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.8953144550323486 \u001b[0m(+0.2851076126098633)\n",
            "     | > avg_loss:\u001b[92m 0.166486918926239 \u001b[0m(-0.00196683406829834)\n",
            "     | > avg_log_mle:\u001b[92m -0.12386035919189453 \u001b[0m(-0.0005518198013305664)\n",
            "     | > avg_loss_dur:\u001b[92m 0.29034727811813354 \u001b[0m(-0.0014150142669677734)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000/best_model_2808.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 37/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:35:38) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 1.3819870948791504 \u001b[0m(+0.48667263984680176)\n",
            "     | > avg_loss:\u001b[91m 0.1727493703365326 \u001b[0m(+0.006262451410293579)\n",
            "     | > avg_log_mle:\u001b[92m -0.12400054931640625 \u001b[0m(-0.00014019012451171875)\n",
            "     | > avg_loss_dur:\u001b[91m 0.29674991965293884 \u001b[0m(+0.006402641534805298)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 38/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:36:19) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:36:33 -- STEP: 6/11 -- GLOBAL_STEP: 2825\u001b[0m\n",
            "     | > loss: 0.16390341520309448  (0.14655438562234244)\n",
            "     | > log_mle: -0.1527320146560669  (-0.15697932243347168)\n",
            "     | > loss_dur: 0.3166354298591614  (0.3035337080558141)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(1.2207, device='cuda:0')  (tensor(1.2090, device='cuda:0'))\n",
            "     | > current_lr: 9.499999999999999e-06 \n",
            "     | > step_time: 0.7615  (1.5381879011789958)\n",
            "     | > loader_time: 0.0076  (0.024144967397054035)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5984456539154053 \u001b[0m(-0.7835414409637451)\n",
            "     | > avg_loss:\u001b[92m 0.17202001810073853 \u001b[0m(-0.0007293522357940674)\n",
            "     | > avg_log_mle:\u001b[92m -0.1243445873260498 \u001b[0m(-0.0003440380096435547)\n",
            "     | > avg_loss_dur:\u001b[92m 0.29636460542678833 \u001b[0m(-0.0003853142261505127)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 39/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:36:51) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv1d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5950145721435547 \u001b[0m(-0.003431081771850586)\n",
            "     | > avg_loss:\u001b[92m 0.16953271627426147 \u001b[0m(-0.0024873018264770508)\n",
            "     | > avg_log_mle:\u001b[91m -0.12423050403594971 \u001b[0m(+0.00011408329010009766)\n",
            "     | > avg_loss_dur:\u001b[92m 0.2937632203102112 \u001b[0m(-0.0026013851165771484)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 40/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:37:21) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:37:35 -- STEP: 9/11 -- GLOBAL_STEP: 2850\u001b[0m\n",
            "     | > loss: 0.1733393669128418  (0.15496810608439976)\n",
            "     | > log_mle: -0.15116655826568604  (-0.15702560212877062)\n",
            "     | > loss_dur: 0.32450592517852783  (0.31199370821317035)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(1.0893, device='cuda:0')  (tensor(1.3897, device='cuda:0'))\n",
            "     | > current_lr: 9.999999999999999e-06 \n",
            "     | > step_time: 0.9065  (1.1051199436187744)\n",
            "     | > loader_time: 0.0075  (0.018737766477796767)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5880303382873535 \u001b[0m(-0.006984233856201172)\n",
            "     | > avg_loss:\u001b[92m 0.16406014561653137 \u001b[0m(-0.0054725706577301025)\n",
            "     | > avg_log_mle:\u001b[92m -0.12461841106414795 \u001b[0m(-0.0003879070281982422)\n",
            "     | > avg_loss_dur:\u001b[92m 0.2886785566806793 \u001b[0m(-0.00508466362953186)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000/best_model_2852.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 41/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:38:00) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5873029232025146 \u001b[0m(-0.0007274150848388672)\n",
            "     | > avg_loss:\u001b[91m 0.16845780611038208 \u001b[0m(+0.004397660493850708)\n",
            "     | > avg_log_mle:\u001b[92m -0.12481105327606201 \u001b[0m(-0.0001926422119140625)\n",
            "     | > avg_loss_dur:\u001b[91m 0.2932688593864441 \u001b[0m(+0.0045903027057647705)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 42/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:38:38) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.6046364307403564 \u001b[0m(+0.017333507537841797)\n",
            "     | > avg_loss:\u001b[92m 0.16627493500709534 \u001b[0m(-0.002182871103286743)\n",
            "     | > avg_log_mle:\u001b[92m -0.12519359588623047 \u001b[0m(-0.00038254261016845703)\n",
            "     | > avg_loss_dur:\u001b[92m 0.2914685308933258 \u001b[0m(-0.0018003284931182861)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 43/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:39:12) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:39:21 -- STEP: 1/11 -- GLOBAL_STEP: 2875\u001b[0m\n",
            "     | > loss: 0.12497055530548096  (0.12497055530548096)\n",
            "     | > log_mle: -0.16495180130004883  (-0.16495180130004883)\n",
            "     | > loss_dur: 0.2899223566055298  (0.2899223566055298)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(0.8837, device='cuda:0')  (tensor(0.8837, device='cuda:0'))\n",
            "     | > current_lr: 1.075e-05 \n",
            "     | > step_time: 2.5622  (2.5622148513793945)\n",
            "     | > loader_time: 0.0501  (0.05005931854248047)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5839526653289795 \u001b[0m(-0.020683765411376953)\n",
            "     | > avg_loss:\u001b[92m 0.15328150987625122 \u001b[0m(-0.012993425130844116)\n",
            "     | > avg_log_mle:\u001b[92m -0.1252816915512085 \u001b[0m(-8.809566497802734e-05)\n",
            "     | > avg_loss_dur:\u001b[92m 0.2785632014274597 \u001b[0m(-0.012905329465866089)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000/best_model_2885.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 44/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:39:56) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.6788909435272217 \u001b[0m(+0.09493827819824219)\n",
            "     | > avg_loss:\u001b[91m 0.16550955176353455 \u001b[0m(+0.012228041887283325)\n",
            "     | > avg_log_mle:\u001b[91m -0.12524652481079102 \u001b[0m(+3.516674041748047e-05)\n",
            "     | > avg_loss_dur:\u001b[91m 0.29075607657432556 \u001b[0m(+0.012192875146865845)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 45/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:40:33) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:40:49 -- STEP: 4/11 -- GLOBAL_STEP: 2900\u001b[0m\n",
            "     | > loss: 0.15517306327819824  (0.13058659434318542)\n",
            "     | > log_mle: -0.15714514255523682  (-0.1615830957889557)\n",
            "     | > loss_dur: 0.31231820583343506  (0.2921696901321411)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(0.9945, device='cuda:0')  (tensor(1.0951, device='cuda:0'))\n",
            "     | > current_lr: 1.125e-05 \n",
            "     | > step_time: 2.422  (2.633490800857544)\n",
            "     | > loader_time: 0.0509  (0.03768128156661987)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5901527404785156 \u001b[0m(-0.08873820304870605)\n",
            "     | > avg_loss:\u001b[92m 0.1570512056350708 \u001b[0m(-0.008458346128463745)\n",
            "     | > avg_log_mle:\u001b[92m -0.12554025650024414 \u001b[0m(-0.000293731689453125)\n",
            "     | > avg_loss_dur:\u001b[92m 0.28259146213531494 \u001b[0m(-0.00816461443901062)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 46/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:41:08) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5847156047821045 \u001b[0m(-0.005437135696411133)\n",
            "     | > avg_loss:\u001b[91m 0.1640763282775879 \u001b[0m(+0.00702512264251709)\n",
            "     | > avg_log_mle:\u001b[92m -0.1261235475540161 \u001b[0m(-0.0005832910537719727)\n",
            "     | > avg_loss_dur:\u001b[91m 0.290199875831604 \u001b[0m(+0.0076084136962890625)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 47/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:41:40) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:41:59 -- STEP: 7/11 -- GLOBAL_STEP: 2925\u001b[0m\n",
            "     | > loss: 0.17001047730445862  (0.14375399265970504)\n",
            "     | > log_mle: -0.1585557460784912  (-0.15941153253827775)\n",
            "     | > loss_dur: 0.32856622338294983  (0.3031655251979828)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(1.2970, device='cuda:0')  (tensor(1.4922, device='cuda:0'))\n",
            "     | > current_lr: 1.1750000000000001e-05 \n",
            "     | > step_time: 0.8095  (1.6973294530596053)\n",
            "     | > loader_time: 0.0071  (0.021546908787318637)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.5909719467163086 \u001b[0m(+0.0062563419342041016)\n",
            "     | > avg_loss:\u001b[91m 0.16652336716651917 \u001b[0m(+0.0024470388889312744)\n",
            "     | > avg_log_mle:\u001b[92m -0.1266242265701294 \u001b[0m(-0.0005006790161132812)\n",
            "     | > avg_loss_dur:\u001b[91m 0.29314759373664856 \u001b[0m(+0.0029477179050445557)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 48/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:42:19) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.5919654369354248 \u001b[0m(+0.000993490219116211)\n",
            "     | > avg_loss:\u001b[92m 0.15656760334968567 \u001b[0m(-0.009955763816833496)\n",
            "     | > avg_log_mle:\u001b[92m -0.12671566009521484 \u001b[0m(-9.143352508544922e-05)\n",
            "     | > avg_loss_dur:\u001b[92m 0.2832832634449005 \u001b[0m(-0.009864330291748047)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 49/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:42:51) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:43:05 -- STEP: 10/11 -- GLOBAL_STEP: 2950\u001b[0m\n",
            "     | > loss: 0.13460469245910645  (0.1475994884967804)\n",
            "     | > log_mle: -0.17974352836608887  (-0.16169605255126954)\n",
            "     | > loss_dur: 0.3143482208251953  (0.3092955410480499)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(2.8828, device='cuda:0')  (tensor(2.0470, device='cuda:0'))\n",
            "     | > current_lr: 1.225e-05 \n",
            "     | > step_time: 0.5407  (1.023856782913208)\n",
            "     | > loader_time: 0.0055  (0.014729118347167969)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.5996286869049072 \u001b[0m(+0.007663249969482422)\n",
            "     | > avg_loss:\u001b[92m 0.15513578057289124 \u001b[0m(-0.0014318227767944336)\n",
            "     | > avg_log_mle:\u001b[92m -0.12682664394378662 \u001b[0m(-0.00011098384857177734)\n",
            "     | > avg_loss_dur:\u001b[92m 0.28196242451667786 \u001b[0m(-0.0013208389282226562)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 50/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:43:21) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 1.448659896850586 \u001b[0m(+0.8490312099456787)\n",
            "     | > avg_loss:\u001b[92m 0.1539362668991089 \u001b[0m(-0.0011995136737823486)\n",
            "     | > avg_log_mle:\u001b[92m -0.12759041786193848 \u001b[0m(-0.0007637739181518555)\n",
            "     | > avg_loss_dur:\u001b[92m 0.28152668476104736 \u001b[0m(-0.00043573975563049316)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 51/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:43:57) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5972075462341309 \u001b[0m(-0.8514523506164551)\n",
            "     | > avg_loss:\u001b[91m 0.1605234444141388 \u001b[0m(+0.006587177515029907)\n",
            "     | > avg_log_mle:\u001b[91m -0.12749290466308594 \u001b[0m(+9.751319885253906e-05)\n",
            "     | > avg_loss_dur:\u001b[91m 0.28801634907722473 \u001b[0m(+0.006489664316177368)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 52/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:44:30) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:44:38 -- STEP: 2/11 -- GLOBAL_STEP: 2975\u001b[0m\n",
            "     | > loss: 0.1303912103176117  (0.12231773138046265)\n",
            "     | > log_mle: -0.17087531089782715  (-0.16927069425582886)\n",
            "     | > loss_dur: 0.30126652121543884  (0.2915884256362915)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(1.4004, device='cuda:0')  (tensor(1.4500, device='cuda:0'))\n",
            "     | > current_lr: 1.3e-05 \n",
            "     | > step_time: 1.7658  (1.704005241394043)\n",
            "     | > loader_time: 0.0184  (0.021151304244995117)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.6091225147247314 \u001b[0m(+0.011914968490600586)\n",
            "     | > avg_loss:\u001b[91m 0.16153603792190552 \u001b[0m(+0.0010125935077667236)\n",
            "     | > avg_log_mle:\u001b[92m -0.12815332412719727 \u001b[0m(-0.0006604194641113281)\n",
            "     | > avg_loss_dur:\u001b[91m 0.2896893620491028 \u001b[0m(+0.0016730129718780518)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 53/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:45:02) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.6088366508483887 \u001b[0m(-0.00028586387634277344)\n",
            "     | > avg_loss:\u001b[92m 0.15584811568260193 \u001b[0m(-0.005687922239303589)\n",
            "     | > avg_log_mle:\u001b[92m -0.12833261489868164 \u001b[0m(-0.000179290771484375)\n",
            "     | > avg_loss_dur:\u001b[92m 0.28418073058128357 \u001b[0m(-0.005508631467819214)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 54/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:45:34) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:45:46 -- STEP: 5/11 -- GLOBAL_STEP: 3000\u001b[0m\n",
            "     | > loss: 0.15700796246528625  (0.13343065977096558)\n",
            "     | > log_mle: -0.1568235158920288  (-0.16321072578430176)\n",
            "     | > loss_dur: 0.31383147835731506  (0.2966413855552673)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(1.9887, device='cuda:0')  (tensor(1.8205, device='cuda:0'))\n",
            "     | > current_lr: 1.35e-05 \n",
            "     | > step_time: 0.9609  (1.6200244903564454)\n",
            "     | > loader_time: 0.0258  (0.022244834899902345)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000/checkpoint_3000.pth\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 1.2179200649261475 \u001b[0m(+0.6090834140777588)\n",
            "     | > avg_loss:\u001b[91m 0.16281500458717346 \u001b[0m(+0.006966888904571533)\n",
            "     | > avg_log_mle:\u001b[92m -0.12905192375183105 \u001b[0m(-0.0007193088531494141)\n",
            "     | > avg_loss_dur:\u001b[91m 0.2918669283390045 \u001b[0m(+0.007686197757720947)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 55/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:46:18) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.7871158123016357 \u001b[0m(-0.4308042526245117)\n",
            "     | > avg_loss:\u001b[92m 0.1609724462032318 \u001b[0m(-0.0018425583839416504)\n",
            "     | > avg_log_mle:\u001b[92m -0.12911152839660645 \u001b[0m(-5.9604644775390625e-05)\n",
            "     | > avg_loss_dur:\u001b[92m 0.29008397459983826 \u001b[0m(-0.0017829537391662598)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 56/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:46:53) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:47:12 -- STEP: 8/11 -- GLOBAL_STEP: 3025\u001b[0m\n",
            "     | > loss: 0.170511394739151  (0.13863785937428474)\n",
            "     | > log_mle: -0.16486620903015137  (-0.16308285295963287)\n",
            "     | > loss_dur: 0.33537760376930237  (0.3017207123339176)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(2.3576, device='cuda:0')  (tensor(2.3440, device='cuda:0'))\n",
            "     | > current_lr: 1.4e-05 \n",
            "     | > step_time: 0.6955  (1.3215722441673279)\n",
            "     | > loader_time: 0.0073  (0.023901671171188354)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.6103425025939941 \u001b[0m(-0.1767733097076416)\n",
            "     | > avg_loss:\u001b[92m 0.15275490283966064 \u001b[0m(-0.008217543363571167)\n",
            "     | > avg_log_mle:\u001b[92m -0.1296004056930542 \u001b[0m(-0.0004888772964477539)\n",
            "     | > avg_loss_dur:\u001b[92m 0.28235530853271484 \u001b[0m(-0.007728666067123413)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000/best_model_3028.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 57/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:47:36) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.6073863506317139 \u001b[0m(-0.0029561519622802734)\n",
            "     | > avg_loss:\u001b[91m 0.15534070134162903 \u001b[0m(+0.002585798501968384)\n",
            "     | > avg_log_mle:\u001b[92m -0.13024687767028809 \u001b[0m(-0.0006464719772338867)\n",
            "     | > avg_loss_dur:\u001b[91m 0.2855875790119171 \u001b[0m(+0.0032322704792022705)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 58/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:48:13) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5886609554290771 \u001b[0m(-0.01872539520263672)\n",
            "     | > avg_loss:\u001b[91m 0.16160470247268677 \u001b[0m(+0.006264001131057739)\n",
            "     | > avg_log_mle:\u001b[92m -0.1303802728652954 \u001b[0m(-0.00013339519500732422)\n",
            "     | > avg_loss_dur:\u001b[91m 0.2919849753379822 \u001b[0m(+0.0063973963260650635)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 59/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:48:46) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:48:50 -- STEP: 0/11 -- GLOBAL_STEP: 3050\u001b[0m\n",
            "     | > loss: 0.038430243730545044  (0.038430243730545044)\n",
            "     | > log_mle: -0.18436706066131592  (-0.18436706066131592)\n",
            "     | > loss_dur: 0.22279730439186096  (0.22279730439186096)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(2.4999, device='cuda:0')  (tensor(2.4999, device='cuda:0'))\n",
            "     | > current_lr: 1.475e-05 \n",
            "     | > step_time: 1.6608  (1.6607816219329834)\n",
            "     | > loader_time: 2.6432  (2.643176317214966)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.6112544536590576 \u001b[0m(+0.02259349822998047)\n",
            "     | > avg_loss:\u001b[92m 0.15401580929756165 \u001b[0m(-0.007588893175125122)\n",
            "     | > avg_log_mle:\u001b[92m -0.13043570518493652 \u001b[0m(-5.543231964111328e-05)\n",
            "     | > avg_loss_dur:\u001b[92m 0.28445151448249817 \u001b[0m(-0.007533460855484009)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 60/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:49:19) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.6759920120239258 \u001b[0m(+0.06473755836486816)\n",
            "     | > avg_loss:\u001b[91m 0.15544578433036804 \u001b[0m(+0.0014299750328063965)\n",
            "     | > avg_log_mle:\u001b[92m -0.13091576099395752 \u001b[0m(-0.0004800558090209961)\n",
            "     | > avg_loss_dur:\u001b[91m 0.28636154532432556 \u001b[0m(+0.0019100308418273926)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 61/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:49:53) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:50:02 -- STEP: 3/11 -- GLOBAL_STEP: 3075\u001b[0m\n",
            "     | > loss: 0.11199221014976501  (0.10979432861010234)\n",
            "     | > log_mle: -0.1605832576751709  (-0.1691890557607015)\n",
            "     | > loss_dur: 0.2725754678249359  (0.27898338437080383)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(4.1132, device='cuda:0')  (tensor(3.3655, device='cuda:0'))\n",
            "     | > current_lr: 1.525e-05 \n",
            "     | > step_time: 1.1721  (1.5111398696899414)\n",
            "     | > loader_time: 0.0226  (0.022476752599080402)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.6159970760345459 \u001b[0m(-0.05999493598937988)\n",
            "     | > avg_loss:\u001b[92m 0.15236154198646545 \u001b[0m(-0.003084242343902588)\n",
            "     | > avg_log_mle:\u001b[91m -0.13088488578796387 \u001b[0m(+3.0875205993652344e-05)\n",
            "     | > avg_loss_dur:\u001b[92m 0.2832464277744293 \u001b[0m(-0.0031151175498962402)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000/best_model_3083.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 62/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:50:31) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5932226181030273 \u001b[0m(-0.022774457931518555)\n",
            "     | > avg_loss:\u001b[91m 0.15600383281707764 \u001b[0m(+0.0036422908306121826)\n",
            "     | > avg_log_mle:\u001b[92m -0.13172078132629395 \u001b[0m(-0.0008358955383300781)\n",
            "     | > avg_loss_dur:\u001b[91m 0.2877246141433716 \u001b[0m(+0.004478186368942261)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 63/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:51:09) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:51:25 -- STEP: 6/11 -- GLOBAL_STEP: 3100\u001b[0m\n",
            "     | > loss: 0.1429820954799652  (0.12642507751782736)\n",
            "     | > log_mle: -0.1612452268600464  (-0.16602780421574911)\n",
            "     | > loss_dur: 0.3042273223400116  (0.2924528817335765)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(1.9432, device='cuda:0')  (tensor(2.2077, device='cuda:0'))\n",
            "     | > current_lr: 1.575e-05 \n",
            "     | > step_time: 0.6186  (1.727272590001424)\n",
            "     | > loader_time: 0.0064  (0.030634442965189617)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.6115174293518066 \u001b[0m(+0.018294811248779297)\n",
            "     | > avg_loss:\u001b[92m 0.14943498373031616 \u001b[0m(-0.006568849086761475)\n",
            "     | > avg_log_mle:\u001b[92m -0.13227951526641846 \u001b[0m(-0.0005587339401245117)\n",
            "     | > avg_loss_dur:\u001b[92m 0.2817144989967346 \u001b[0m(-0.006010115146636963)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000/best_model_3105.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 64/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:51:45) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.9926092624664307 \u001b[0m(+0.381091833114624)\n",
            "     | > avg_loss:\u001b[92m 0.14376598596572876 \u001b[0m(-0.005668997764587402)\n",
            "     | > avg_log_mle:\u001b[91m -0.13215398788452148 \u001b[0m(+0.00012552738189697266)\n",
            "     | > avg_loss_dur:\u001b[92m 0.27591997385025024 \u001b[0m(-0.005794525146484375)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000/best_model_3116.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 65/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:52:30) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:52:51 -- STEP: 9/11 -- GLOBAL_STEP: 3125\u001b[0m\n",
            "     | > loss: 0.14526012539863586  (0.1337890326976776)\n",
            "     | > log_mle: -0.15924739837646484  (-0.16563679112328422)\n",
            "     | > loss_dur: 0.3045075237751007  (0.29942582382096183)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(2.8147, device='cuda:0')  (tensor(2.9737, device='cuda:0'))\n",
            "     | > current_lr: 1.625e-05 \n",
            "     | > step_time: 0.7775  (1.5280020766788058)\n",
            "     | > loader_time: 0.0064  (0.019167794121636286)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.5890250205993652 \u001b[0m(-0.40358424186706543)\n",
            "     | > avg_loss:\u001b[91m 0.1442832350730896 \u001b[0m(+0.0005172491073608398)\n",
            "     | > avg_log_mle:\u001b[92m -0.13222074508666992 \u001b[0m(-6.67572021484375e-05)\n",
            "     | > avg_loss_dur:\u001b[91m 0.2765039801597595 \u001b[0m(+0.0005840063095092773)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 66/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:53:09) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.650470495223999 \u001b[0m(+0.06144547462463379)\n",
            "     | > avg_loss:\u001b[91m 0.1469622552394867 \u001b[0m(+0.0026790201663970947)\n",
            "     | > avg_log_mle:\u001b[92m -0.13293921947479248 \u001b[0m(-0.0007184743881225586)\n",
            "     | > avg_loss_dur:\u001b[91m 0.2799014747142792 \u001b[0m(+0.0033974945545196533)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 67/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:53:43) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.6241376399993896 \u001b[0m(-0.026332855224609375)\n",
            "     | > avg_loss:\u001b[92m 0.13371491432189941 \u001b[0m(-0.01324734091758728)\n",
            "     | > avg_log_mle:\u001b[92m -0.13323307037353516 \u001b[0m(-0.0002938508987426758)\n",
            "     | > avg_loss_dur:\u001b[92m 0.26694798469543457 \u001b[0m(-0.012953490018844604)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000/best_model_3149.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 68/250\u001b[0m\n",
            " --> /content/drive/MyDrive/ljspeech-002/tts_train_dir/run-June-21-2024_12+06PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2024-06-21 12:54:21) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2024-06-21 12:54:30 -- STEP: 1/11 -- GLOBAL_STEP: 3150\u001b[0m\n",
            "     | > loss: 0.09258395433425903  (0.09258395433425903)\n",
            "     | > log_mle: -0.17499899864196777  (-0.17499899864196777)\n",
            "     | > loss_dur: 0.2675829529762268  (0.2675829529762268)\n",
            "     | > amp_scaler: 65536.0  (65536.0)\n",
            "     | > grad_norm: tensor(1.4471, device='cuda:0')  (tensor(1.4471, device='cuda:0'))\n",
            "     | > current_lr: 1.7e-05 \n",
            "     | > step_time: 2.234  (2.233950614929199)\n",
            "     | > loader_time: 0.0484  (0.04841208457946777)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.9279077053070068 \u001b[0m(+0.3037700653076172)\n",
            "     | > avg_loss:\u001b[92m 0.13156896829605103 \u001b[0m(-0.0021459460258483887)\n",
            "     | > avg_log_mle:\u001b[92m -0.13351547718048096 \u001b[0m(-0.0002824068069458008)\n",
            "     | > avg_loss_dur:\u001b[92m 0.265084445476532 \u001b[0m(-0.0018635392189025879)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# start the training process\n",
        "trainer.fit()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5228410,
          "sourceId": 8715008,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30732,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
